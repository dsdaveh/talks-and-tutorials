---
title: "Keras MNIST Hello world"
output: html_notebook
---



```{r}
library(keras)
# install_keras()
```

```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y 
x_test <- mnist$test$x
y_test <- mnist$test$y
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
tibble::glimpse(x_train)
tibble::glimpse(y_train)
range(x_train)
```

```{r}
x_train[1,,]  %>% apply(2,rev) %>% t() %>% image(col = gray(0:255 / 255), asp=1)
```

```{r}
# reshape and rescale
x_train <- x_train / 255
x_test <- x_test / 255

x_train_vec <- array_reshape(x_train, c(nrow(x_train), 784))
x_test_vec <- array_reshape(x_test, c(nrow(x_test), 784))

glimpse(x_train_vec)
```


```{r}
glimpse(y_train)
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

dim(y_train)
y_train[1:10, ]
```

```{r}
# defining the model and layers
model <- keras_model_sequential() 
model %>%
  layer_dense(units = 256, activation = 'relu',  input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')
model
```

```{r}
# compile (define loss and optimizer)
model %>% compile(
  loss = 'categorical_crossentropy', 
  optimizer = optimizer_rmsprop(), 
  metrics = c('accuracy')
)
```

```{r}
# train (fit)
history <- model %>% fit(
  x_train_vec, y_train,
  epochs = 30, batch_size = 128, 
  validation_split = 0.2,
  verbose = 2
)
```

```{r}
model %>% evaluate(x_test_vec, y_test) 
```

```{r}
model %>% predict_classes(x_test_vec) %>% head(10)
```
```{r}
plot(history)
```


```{r}
# reshape vor conv net
x_train_conv <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test_conv <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

glimpse(x_train_conv)
```

```{r}
model2 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = c(28,28,1)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')
model2
```


```{r}
# compile (define loss and optimizer)
model2 %>% compile(
  loss = 'categorical_crossentropy', 
  optimizer = optimizer_rmsprop(), 
  metrics = c('accuracy')
)
```

```{r}
# train (fit)
history2 <- model2 %>% fit(
  x_train_conv, y_train,
  epochs = 15, batch_size = 128, 
  validation_split = 0.2
)
```

```{r}
model2 %>% evaluate(x_test_conv, y_test, verbose=2) 
```

```{r}
# saveRDS(history2, 'history2.RDS')
# history2 <- readRDS('history2.RDS')
plot(history2)
```

