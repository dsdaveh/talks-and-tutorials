---
title: "Intro to sparklyr"
output: html_notebook
---
## 6.1 -  New Spark session

1. Use `spark_connect()` to create a new local Spark session
```{r}
library(tidyverse)
library(sparklyr)

##spark_install(version = "2.0.0")
sc <- spark_connect(master = "local", version = "2.0.0")
```

2. Click on the `SparkUI` button to view the current Spark session's UI

3. Click on the `Log` button to see the message history

## 6.2 - Data transfer

1. Copy the `mtcars` dataset into the session
```{r}
spark_mtcars <- sdf_copy_to(sc, mtcars, "my_mtcars")
```

2. In the **Connections** pane, expande the `my_mtcars` table

3. Go to the Spark UI, note the new jobs

4. In the UI, click the Storage button, note the new table

5. Click on the **In-memory table my_mtcars** link

## 6.3 - Simple dplyr example

1. Run the following code snipett
```{r}
spark_mtcars %>%
  group_by(am) %>%
  summarise(avg_wt = mean(wt, na.rm = TRUE))
```

2. Go to the Spark UI and click the **SQL** button 

3. Click on the top item inside the **Completed Queries** table

4. At the bottom of the diagram, expand **Details**

## 6.4 - Map data

1. Examine the contents of the /usr/share/flights/data folder

2. Read the top 5 rows of the `flight_2008_1` CSV file.  It is located under /usr/share/flights
```{r}
library(readr)
top_rows <- read.csv("/usr/share/flights/data/flight_2008_1.csv", nrows = 5)
```

3. Create a list based on the column names, and add a list item with "character" as its value.
```{r}
library(purrr)
file_columns <- top_rows %>%
  rename_all(tolower) %>%
  map(function(x) "character")
head(file_columns)
```

4. Use `spark_read()` to "map" the file's structure and location to the Spark context
```{r}
spark_flights <- spark_read_csv(
  sc,
  name = "flights",
  path = "/usr/share/flights/data/",
  memory = FALSE,
  columns = file_columns,
  infer_schema = FALSE
)
```

5. In the Connections pane, click on the table icon by the `flights` variable

6. Verify that the new variable pointer work using `tally()`
```{r, message=FALSE, warning=FALSE}
library(tictoc)
tocsh <- function() toc(quiet = TRUE, log = TRUE)

tic("uncached tally")

spark_flights %>%
  tally()

tocsh()
```

## 6.5 -  Caching data

1. Create a subset of the *flights* table object
```{r}
cached_flights <- spark_flights %>%
  mutate(
    arrdelay = ifelse(arrdelay == "NA", 0, arrdelay),
    depdelay = ifelse(depdelay == "NA", 0, depdelay)
  ) %>%
  select(
    month,
    dayofmonth,
    arrtime,
    arrdelay,
    depdelay,
    crsarrtime,
    crsdeptime,
    distance
  ) %>%
  mutate_at(vars(month:distance), as.numeric)
```

2. Use `compute()` to extract the data into Spark memory
```{r}
cached_flights <- compute(cached_flights, "sub_flights")
```

3. Confirm new variable pointer works
```{r}
tic('cached tally')

cached_flights %>%
  tally()

tocsh()
```

```{r}
tic.log() %>% unlist() %>% writeLines()
tic.clearlog()
```
## 6.6 - sdf Functions

Consider a simple pivot. First in R locally using tidyr::spread

```{r}
tic('pivot local')
cached_flights %>%
  group_by(month, dayofmonth) %>%
  tally() %>%
  collect() %>% 
  spread(dayofmonth, n)

tocsh()
```

http://spark.rstudio.com/reference/#section-spark-dataframes 

1. Use `sdf_pivot` to create a column for each value in month
```{r}
tic('pivot Spark')
cached_flights %>%
  mutate(
    month = paste0("M", as.integer(month)),
    dayofmonth = paste0("D", as.integer(dayofmonth))
    ) %>%
  sdf_pivot(month~dayofmonth) 

tocsh()
```

```{r}
tic.log() %>% unlist() %>% writeLines()
tic.clearlog()
```

2. Use `sdf_partition()` to sepparate the data into discrete groups
```{r}
partition <- cached_flights %>%
  sdf_partition(training = 0.01, testing = 0.09, other = 0.9)

tally(partition$training)
```

## 6.7 - Feature transformers
http://spark.rstudio.com/reference/#section-spark-feature-transformers

1. Use `ft_binarizer()` to identify "delayed" flights
```{r}
cached_flights %>%
  ft_binarizer(
    input.col = "depdelay",
    output.col = "delayed",
    threshold = 15
  ) %>%
  select(
    depdelay,
    delayed
  ) %>%
  head(100)
```

2. Use `ft_bucketizer()` to split the data into groups
```{r}
cached_flights %>%
  ft_bucketizer(
    input.col = "crsdeptime",
    output.col = "dephour",
    splits = c(0, 400, 800, 1200, 1600, 2000, 2400)
  ) %>%
  select(
    crsdeptime,
    dephour
  ) %>%
  head(100)
```

## 6.8 - Fit a model with sparklyr

1. Combine the `ft_` and `sdf_` functions to prepare the data
```{r}
sample_data <- cached_flights %>%
  filter(!is.na(arrdelay)) %>%
  ft_binarizer(
    input.col = "arrdelay",
    output.col = "delayed",
    threshold = 15
  ) %>%
  ft_bucketizer(
    input.col = "crsdeptime",
    output.col = "dephour",
    splits = c(0, 400, 800, 1200, 1600, 2000, 2400)
  ) %>%
  mutate(dephour = paste0("h", as.integer(dephour))) %>%
  sdf_partition(training = 0.01, testing = 0.09, other = 0.9)
```

2. Cache the training data
```{r}
training <- sdf_register(sample_data$training, "training")
tbl_cache(sc, "training")

```

3. Run a logistic regression model in Spark
```{r}
delayed_model <- training %>%
  ml_logistic_regression(delayed ~ depdelay + dephour)
```

4. View the model results
```{r}
summary(delayed_model)
```

## 6.9 - Run predictions in Spark

1. Use `sdf_predict()` agains the test dataset
```{r}
delayed_testing <- sdf_predict(delayed_model, sample_data$testing)
delayed_testing %>%
  head()
```

2. Use `group_by()` to see how effective the new model is
```{r}
delayed_testing %>%
  group_by(delayed, prediction) %>%
  tally()
```

## Before we go

Remember our complex query from Postgres? What would it take to port that to Spark?
```{r}

complex_spark <- spark_flights %>%
  mutate(
    arrdelay = ifelse(arrdelay == "NA", 0, arrdelay),
    depdelay = ifelse(depdelay == "NA", 0, depdelay)
  ) %>%
  select(
    uniquecarrier,
    dest,
    month,
    dayofmonth,
    arrtime,
    arrdelay,
    depdelay,
    crsarrtime,
    crsdeptime,
    distance
  ) %>%
  mutate_at(vars(month:distance), as.numeric) %>%
  compute("sub_flights2")

```
```{r}
complex_spark %>%
  filter(distance > 100) %>%
  group_by(uniquecarrier, dest, month) %>%
  tally() %>%
  filter(n > 10) %>%
  group_by(uniquecarrier, dest) %>%
  summarize(n_avg = mean(n, na.rm = T) %>% as.integer()) %>%
  arrange(desc(n_avg))
```


```{r, include = FALSE}
spark_disconnect(sc)
```

